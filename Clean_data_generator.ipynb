{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a623cb30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/lisalaurent/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/lisalaurent/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import bz2\n",
    "import json\n",
    "from tld import get_tld\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import string\n",
    "import math\n",
    "from collections import Counter\n",
    "from operator import itemgetter\n",
    "import re\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f981d7f7",
   "metadata": {},
   "source": [
    "### CREATE PATHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d4ca78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = './Data/'\n",
    "FILE2016 = DATA_PATH + 'quotes-2016.json.bz2'\n",
    "PATH_OUT = DATA_PATH + 'clean-quotes-2016.json.bz2'\n",
    "#FILE2020 = DATA_PATH + 'quotes-2020.json.bz2'\n",
    "#df_base = pd.read_json(FILE2016, lines=True, compression='bz2', nrows=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09144038",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_base = pd.read_json(FILE2016, lines=True, compression='bz2', nrows=100)\n",
    "df_base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa3fec4",
   "metadata": {},
   "source": [
    "## Create functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b46a66b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sitename(url):\n",
    "    res = get_tld(url, as_object=True)\n",
    "    return res.domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1db25fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_name(df): \n",
    "    df['sitenames'] = np.nan\n",
    "    for row in range(df.shape[0]):\n",
    "        df['sitenames'][row] = [get_sitename(site) for site in df['urls'][row]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4552663",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_domain(url):\n",
    "    res = get_tld(url, as_object=True)\n",
    "    return res.tld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20b06709",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sitename(url):\n",
    "    res = get_tld(url, as_object=True)\n",
    "    return res.domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d787f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "def tokenize(text):\n",
    "    text = \"\".join([ch for ch in text if ch not in string.punctuation]) # get the text without punctuation\n",
    "    tokens = nltk.word_tokenize(text) #Tokenizers divide strings into lists of substrings\n",
    "    return \" \".join([stemmer.stem(word.lower()) for word in tokens if word not in stopwords.words('english')]) #stem all words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6969768",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generalizeDictionary(matchers):\n",
    "    for category in matchers:\n",
    "        for i in range(len(matchers[category])):\n",
    "            matchers[category][i] = tokenize(matchers[category][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99e24047",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordsFromURL(url):\n",
    "    return re.compile(r'\\W+',re.UNICODE).split(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da135064",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(matchers, url): #Give it an already generalized dictionary\n",
    "    tag_found = []\n",
    "    general_url = [tokenize(x) for x in getWordsFromURL(url)]\n",
    "    for category in matchers:\n",
    "        for i in range(len(matchers[category])):\n",
    "            match = matchers[category][i]\n",
    "            if match in general_url:\n",
    "                tag_found.append(category)\n",
    "    return tag_found # or you can \"return None\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f24324b",
   "metadata": {},
   "source": [
    "### PROCESSING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8d2c920",
   "metadata": {},
   "outputs": [],
   "source": [
    "matchers = {\"art\": [\"art\", \"paint\", \"draw\"], \"business\": [\"business\", \"finance\", \"economy\"], \"sport\": [\"sport\", \"football\"]}\n",
    "generalizeDictionary(matchers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f8610fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/j9/h2f8ljxj1q15x5vkfkkmdz_80000gn/T/ipykernel_23371/2829134936.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mbz2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH_OUT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0md_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_reader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0mchunk_cleaned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_chunk_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m             \u001b[0mchunk_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchunk_cleaned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'columns'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0;31m#d_file.write((json.dumps(chunk_cleaned)+'\\n').encode('utf-8'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/j9/h2f8ljxj1q15x5vkfkkmdz_80000gn/T/ipykernel_23371/2829134936.py\u001b[0m in \u001b[0;36mprocess_chunk_complete\u001b[0;34m(chunk, threshold_probas)\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mtld\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_domain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_sitename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m                 \u001b[0mcategories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatchers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m                 \u001b[0;31m#Append data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0mdomains\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtld\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/j9/h2f8ljxj1q15x5vkfkkmdz_80000gn/T/ipykernel_23371/2130975852.py\u001b[0m in \u001b[0;36mclassify\u001b[0;34m(matchers, url)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mclassify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatchers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#Give it an already generalized dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mtag_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mgeneral_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgetWordsFromURL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcategory\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmatchers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatchers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcategory\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/j9/h2f8ljxj1q15x5vkfkkmdz_80000gn/T/ipykernel_23371/2130975852.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mclassify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatchers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#Give it an already generalized dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mtag_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mgeneral_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgetWordsFromURL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcategory\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmatchers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatchers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcategory\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/j9/h2f8ljxj1q15x5vkfkkmdz_80000gn/T/ipykernel_23371/848604076.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mch\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mch\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# get the text without punctuation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Tokenizers divide strings into lists of substrings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#stem all words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/j9/h2f8ljxj1q15x5vkfkkmdz_80000gn/T/ipykernel_23371/848604076.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mch\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mch\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# get the text without punctuation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Tokenizers divide strings into lists of substrings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#stem all words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Applications/anaconda3/envs/adaproject/lib/python3.9/site-packages/nltk/corpus/reader/wordlist.py\u001b[0m in \u001b[0;36mwords\u001b[0;34m(self, fileids, ignore_lines_startswith)\u001b[0m\n\u001b[1;32m     19\u001b[0m         return [\n\u001b[1;32m     20\u001b[0m             \u001b[0mline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mline_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_lines_startswith\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         ]\n",
      "\u001b[0;32m/Applications/anaconda3/envs/adaproject/lib/python3.9/site-packages/nltk/corpus/reader/api.py\u001b[0m in \u001b[0;36mraw\u001b[0;34m(self, fileids)\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mcontents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfileids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m                 \u001b[0mcontents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/adaproject/lib/python3.9/site-packages/nltk/corpus/reader/api.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, file)\u001b[0m\n\u001b[1;32m    228\u001b[0m         \"\"\"\n\u001b[1;32m    229\u001b[0m         \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_root\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/adaproject/lib/python3.9/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, encoding)\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m             \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSeekableUnicodeStreamReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/adaproject/lib/python3.9/site-packages/nltk/compat.py\u001b[0m in \u001b[0;36m_decorator\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpy3_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_decorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_py3_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minit_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/adaproject/lib/python3.9/site-packages/nltk/compat.py\u001b[0m in \u001b[0;36madd_py3_data\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0madd_py3_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_PY3_DATA_UPDATES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"/PY3\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\".zip\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def process_chunk_complete(chunk, threshold_probas):\n",
    "    print(f'Processing chunk with {len(chunk)} rows')\n",
    "    #DATA CLEANING\n",
    "    #Remove None speakers\n",
    "    chunk = chunk.drop(chunk[chunk['speaker']=='None'].index)\n",
    "    chunk = chunk.reset_index(drop=True)\n",
    "    \n",
    "    #Remove none unique ids and keep the first one\n",
    "    if not(chunk.quoteID.is_unique):\n",
    "        chunk = chunk.drop_duplicates(subset=['quoteID'], keep='first')\n",
    "        \n",
    "    #Remove nan or empty quotes \n",
    "    chunk = chunk.drop(chunk[chunk[\"quotation\"].isna() | chunk[\"quotation\"].isnull()].index)\n",
    "    chunk = chunk.reset_index(drop=True)\n",
    "\n",
    "    #Remove speakers for which probability is lower than a threshold\n",
    "        #Gather the first probability for each row\n",
    "    string_probas, nber_probas = [], []\n",
    "    string_probas = [chunk['probas'][i][0][1] for i in chunk.index]\n",
    "    nber_probas = list(map(float, string_probas))\n",
    "    series_probas = pd.Series(nber_probas, dtype='float64', index=chunk.index)\n",
    "        #Check if the probability is larger than the threshold, if not remove corresponding index\n",
    "    chunk = chunk.drop(series_probas[series_probas < threshold_probas].index)\n",
    "    chunk = chunk.reset_index(drop=True)\n",
    "    \n",
    "    #URLS DATA EXTRACTION\n",
    "    tags_column = [] # List of lists. Each list inside corresponds to a row. Will become the 'tag' column\n",
    "    site_column = []\n",
    "    domain_column = []\n",
    "    for index, row in chunk.iterrows():\n",
    "            tags = [] #tags for all the urls in that row\n",
    "            domains = []\n",
    "            sitenames = []\n",
    "            for url in row['urls']:\n",
    "                #Extract data\n",
    "                tld = get_domain(url)\n",
    "                name = get_sitename(url)\n",
    "                categories = classify(matchers, url)\n",
    "                #Append data\n",
    "                domains.append(tld)\n",
    "                sitenames.append(name)\n",
    "                tags.append(categories)\n",
    "            tags_column.append(tags)\n",
    "            site_column.append(sitenames)\n",
    "            domain_column.append(domains)\n",
    "    # Create new columns with new data\n",
    "    chunk['sitenames'] = site_column\n",
    "    chunk['domain'] = domain_column\n",
    "    chunk['tags'] = tags_column\n",
    "    \n",
    "    tot_length = len(chunk)\n",
    "    return chunk, tot_length\n",
    "\n",
    "with pd.read_json(FILE2016, lines=True, compression='bz2', chunksize=1000) as df_reader:\n",
    "    with bz2.open(PATH_OUT, 'wb') as d_file:\n",
    "        for chunk in df_reader:\n",
    "            chunk_cleaned, chunk_length = process_chunk_complete(chunk, 0.5)\n",
    "            chunk_json = chunk_cleaned.to_json(orient='columns')\n",
    "            #d_file.write((json.dumps(chunk_cleaned)+'\\n').encode('utf-8'))\n",
    "            d_file.write((chunk_json+'\\n').encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f79aeb76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quoteID</th>\n",
       "      <th>quotation</th>\n",
       "      <th>speaker</th>\n",
       "      <th>qids</th>\n",
       "      <th>date</th>\n",
       "      <th>numOccurrences</th>\n",
       "      <th>probas</th>\n",
       "      <th>urls</th>\n",
       "      <th>phase</th>\n",
       "      <th>sitenames</th>\n",
       "      <th>domain</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'0': '2016-12-26-000040', '1': '2016-07-31-00...</td>\n",
       "      <td>{'0': '[ ] and Chris [ Jones ] were in there a...</td>\n",
       "      <td>{'0': 'Andy Reid', '1': 'Mike Howe', '2': 'Hil...</td>\n",
       "      <td>{'0': ['Q2622812', 'Q27830815', 'Q470738', 'Q4...</td>\n",
       "      <td>{'0': 1482782700000, '1': 1469953332000, '2': ...</td>\n",
       "      <td>{'0': 1, '1': 2, '2': 1, '3': 1, '4': 1, '5': ...</td>\n",
       "      <td>{'0': [['Andy Reid', '0.9432'], ['None', '0.05...</td>\n",
       "      <td>{'0': ['http://www.kcchiefs.com/news/article-2...</td>\n",
       "      <td>{'0': 'E', '1': 'E', '2': 'E', '3': 'E', '4': ...</td>\n",
       "      <td>{'0': ['kcchiefs'], '1': ['peninsuladailynews'...</td>\n",
       "      <td>{'0': ['com'], '1': ['com', 'com'], '2': ['com...</td>\n",
       "      <td>{'0': [[]], '1': [[], []], '2': [[]], '3': [[]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'0': '2016-09-16-022093', '1': '2016-08-05-01...</td>\n",
       "      <td>{'0': 'everyone knows the professional hatred ...</td>\n",
       "      <td>{'0': 'DeAngelo Williams', '1': 'Karen Olson',...</td>\n",
       "      <td>{'0': ['Q3020040'], '1': ['Q6369926'], '2': ['...</td>\n",
       "      <td>{'0': 1474044154000, '1': 1470355200000, '2': ...</td>\n",
       "      <td>{'0': 1, '1': 1, '2': 29, '3': 1, '4': 1, '5':...</td>\n",
       "      <td>{'0': [['DeAngelo Williams', '0.9444'], ['None...</td>\n",
       "      <td>{'0': ['http://wcpo.com/sports/football/bengal...</td>\n",
       "      <td>{'0': 'E', '1': 'E', '2': 'E', '3': 'E', '4': ...</td>\n",
       "      <td>{'0': ['wcpo'], '1': ['kuow'], '2': ['csmonito...</td>\n",
       "      <td>{'0': ['com'], '1': ['org'], '2': ['com', 'com...</td>\n",
       "      <td>{'0': [['sport', 'sport']], '1': [[]], '2': [[...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             quoteID  \\\n",
       "0  {'0': '2016-12-26-000040', '1': '2016-07-31-00...   \n",
       "1  {'0': '2016-09-16-022093', '1': '2016-08-05-01...   \n",
       "\n",
       "                                           quotation  \\\n",
       "0  {'0': '[ ] and Chris [ Jones ] were in there a...   \n",
       "1  {'0': 'everyone knows the professional hatred ...   \n",
       "\n",
       "                                             speaker  \\\n",
       "0  {'0': 'Andy Reid', '1': 'Mike Howe', '2': 'Hil...   \n",
       "1  {'0': 'DeAngelo Williams', '1': 'Karen Olson',...   \n",
       "\n",
       "                                                qids  \\\n",
       "0  {'0': ['Q2622812', 'Q27830815', 'Q470738', 'Q4...   \n",
       "1  {'0': ['Q3020040'], '1': ['Q6369926'], '2': ['...   \n",
       "\n",
       "                                                date  \\\n",
       "0  {'0': 1482782700000, '1': 1469953332000, '2': ...   \n",
       "1  {'0': 1474044154000, '1': 1470355200000, '2': ...   \n",
       "\n",
       "                                      numOccurrences  \\\n",
       "0  {'0': 1, '1': 2, '2': 1, '3': 1, '4': 1, '5': ...   \n",
       "1  {'0': 1, '1': 1, '2': 29, '3': 1, '4': 1, '5':...   \n",
       "\n",
       "                                              probas  \\\n",
       "0  {'0': [['Andy Reid', '0.9432'], ['None', '0.05...   \n",
       "1  {'0': [['DeAngelo Williams', '0.9444'], ['None...   \n",
       "\n",
       "                                                urls  \\\n",
       "0  {'0': ['http://www.kcchiefs.com/news/article-2...   \n",
       "1  {'0': ['http://wcpo.com/sports/football/bengal...   \n",
       "\n",
       "                                               phase  \\\n",
       "0  {'0': 'E', '1': 'E', '2': 'E', '3': 'E', '4': ...   \n",
       "1  {'0': 'E', '1': 'E', '2': 'E', '3': 'E', '4': ...   \n",
       "\n",
       "                                           sitenames  \\\n",
       "0  {'0': ['kcchiefs'], '1': ['peninsuladailynews'...   \n",
       "1  {'0': ['wcpo'], '1': ['kuow'], '2': ['csmonito...   \n",
       "\n",
       "                                              domain  \\\n",
       "0  {'0': ['com'], '1': ['com', 'com'], '2': ['com...   \n",
       "1  {'0': ['com'], '1': ['org'], '2': ['com', 'com...   \n",
       "\n",
       "                                                tags  \n",
       "0  {'0': [[]], '1': [[], []], '2': [[]], '3': [[]...  \n",
       "1  {'0': [['sport', 'sport']], '1': [[]], '2': [[...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_base = pd.read_json(DATA_PATH + 'clean-quotes-2016.json.bz2', lines=True, compression='bz2', nrows=3000)\n",
    "df_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd636eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
