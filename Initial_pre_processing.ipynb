{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6119acc8-475d-4232-8309-e84c626355b9",
   "metadata": {},
   "source": [
    "## Initial pre-processing\n",
    "\n",
    "In this notebook we will:\n",
    "- Proceed with basic cleaning of each year from 2015-2020 which includes remove nan speakers, nan quotes and further inconsistencies \n",
    "- Exploit the useful information from the wikidata dumps provided to us \n",
    "- Add columns of interest for further analysis \n",
    "\n",
    "The output will be pickle files for each year with additional columns such as tags, gender (male/female), domain name, citizenship of spokesperson...\n",
    "\n",
    "\n",
    "#### Useful libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b83648-ee02-4be9-8280-923761c459b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm.auto import trange, tqdm\n",
    "import time\n",
    "from journal_API_wikidata import extract_info_wiki\n",
    "from Data_clean_functions import *\n",
    "from tld import get_tld\n",
    "\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b959ddd-4327-46bd-b33c-34d311e5629f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whether to run cleaning or not\n",
    "RUN_CLEANING = False\n",
    "# Note: Approximate 4 hours per year\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c23ec22-509a-417d-a94a-460ac5ac5d84",
   "metadata": {},
   "source": [
    "### Data paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fee7d63-8c3b-4f88-b6c8-7d7dfb1218d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Please note the data is not provided but the `FILE` represents the \n",
    "    data given by QuoteBank \n",
    "\"\"\"\n",
    "DATA_PATH = './data/'\n",
    "FILE = DATA_PATH + 'quotes-2020.json.bz2'\n",
    "PATH_OUT = DATA_PATH + 'rapid_clean-quotes-2020.json.bz2'\n",
    "PATH_OUT_filter = DATA_PATH + 'filter_clean-quotes-2020.json.bz2'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89db466e-dda2-4bdf-ada8-4549dbc8ca28",
   "metadata": {},
   "source": [
    "#### Read file, clean and save in pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a323b368-06b6-41d8-89d0-3c2aa85ff561",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_chunks = 0\n",
    "Total_count = Counter()\n",
    "top_sites = []\n",
    "\n",
    "with pd.read_json(FILE, lines=True, compression='bz2', chunksize=100000) as df_reader:\n",
    "    for chunk in tqdm(df_reader):\n",
    "\n",
    "        # Basic cleaning (refer to function doc)\n",
    "        df_base_clean = rapid_clean(chunk)\n",
    "        \n",
    "        # Extract site name from dataframe\n",
    "        extract_name(df_base_clean)\n",
    "        \n",
    "        # Expand the df on sitenames and urls\n",
    "        df_base_clean_exp = df_base_clean.explode([\"sitenames\", \"urls\"])\n",
    "        \n",
    "        # Save chunk by chunk appending the clean df\n",
    "        with open(PATH_OUT, 'ab') as d_file:\n",
    "            pickle.dump(df_base_clean_exp, d_file)\n",
    "            n_chunks += 1\n",
    "            \n",
    "        # Add counter for occurences of a specific media\n",
    "        counts = Counter(df_base_clean_exp['sitenames'].tolist()) \n",
    "        Total_count += counts\n",
    "        print(\"Chunk done\")\n",
    "   \n",
    "    # List the top 100 most occuring media\n",
    "    for site, count in Total_count.most_common(100):\n",
    "            top_sites.append(site)\n",
    "            \n",
    "\n",
    "    \n",
    "print(\"finished top sites\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766b2ac2-38a3-475c-b6c1-05c1322db37b",
   "metadata": {},
   "source": [
    "#### Pickle save the top_sites for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776b94de-6e70-4a74-9e4e-afcb20d7a525",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_PATH + 'top_sites.pkl', 'wb') as output:\n",
    "    pickle.dump(top_sites, output)\n",
    "\n",
    "\n",
    "# Pickle open the top_sites\n",
    "\"\"\"\n",
    "with open(DATA_PATH + 'top_sites.pkl', 'rb') as file:\n",
    "    top_sites = pickle.load(file)\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ad14f2-d8cb-4d8a-8916-a213a5c6a788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Milestone 2 keep 10 most citing media\n",
    "\n",
    "\"\"\"\n",
    "    Note: We intend on increasing the number from 10 to 50 but this would take \n",
    "    an approximate 5 hours to run per year, we thus plan on running it after the deadline \n",
    "    and focused on making a main pipeline first\n",
    "\"\"\"\n",
    "top_10_sites = top_sites[:10]\n",
    "top_10_sites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6276d00e-45c2-4db7-a2db-6a26f3d4887d",
   "metadata": {},
   "source": [
    "### Filter the rows belonging to top 10 sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bc3451-7826-4b7e-a742-da4e4b413202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New df with rows belonging to top 10 sites\n",
    "\n",
    "chunks_all_filtered = pd.DataFrame(columns=['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
    "       'probas', 'urls', 'phase', 'sitenames'])\n",
    "\n",
    "chunk_nbr = 0\n",
    "n_chunks_filtered = 0\n",
    "\n",
    "with open(PATH_OUT, 'rb') as d_file:\n",
    "    while (chunk_nbr < n_chunks):\n",
    "        \n",
    "        # Progress meter\n",
    "        print(f\"{chunk_nbr}/{n_chunks}\")\n",
    "        \n",
    "        chunk = pickle.load(d_file)\n",
    "        \n",
    "        # Filter chunks with sitenames belonging to top 10\n",
    "        chunk_filtered = chunk[chunk.sitenames.isin(top_10_sites)]\n",
    "        \n",
    "        # Save filtered chunks\n",
    "        with open(PATH_OUT_filter, 'ab') as d_file_out:\n",
    "            pickle.dump(chunk_filtered, d_file_out)\n",
    "            n_chunks_filtered += 1\n",
    "            \n",
    "        chunks_all_filtered = chunks_all_filtered.append(chunk_filtered)\n",
    "\n",
    "        chunk_nbr+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2304ce04-3688-4afc-9d35-653af9eaf7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as pickle for future use\n",
    "\n",
    "with open(DATA_PATH + 'chunks_all_filtered.pkl', 'wb') as output:\n",
    "    pickle.dump(chunks_all_filtered, output)\n",
    "\n",
    "\"\"\"\n",
    "    # Open pickled dataframe\n",
    "with open(DATA_PATH +'chunks_all_filtered.pkl', 'rb') as output:\n",
    "    chunks_all_filtered = pickle.load(output)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ab4344-0f5a-4fa3-a2b7-f1e0ba3c3113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groupby the exploded data set \n",
    "gb_all_filtered = chunks_all_filtered[[\"speaker\", \"qids\" , \"urls\", \"quoteID\", \"quotation\",\"date\"]].groupby([\"speaker\", \"qids\", \"quoteID\", \"quotation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b91404d-efaa-495c-a467-c0f8e0fe1465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One row, quote, may be cited by different media so we list them\n",
    "df_filtered = gb_all_filtered[\"urls\"].apply(list)\n",
    "\n",
    "df_filtered_final = df_filtered.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d7ca73-9fae-454e-a21f-964bdace57a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save pickled dataframe\n",
    "with open(DATA_PATH + 'df_filtered_final.pkl', 'wb') as output:\n",
    "    pickle.dump(df_filtered_final, output)\n",
    "    \n",
    "'''# Open pickled dataframe\n",
    "with open(DATA_PATH + 'df_filtered_final.pkl', 'rb') as output:\n",
    "    df_filtered = pickle.load(output)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468e22fb-c9b5-4f1d-b96d-d48773823ec2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create a dictionnary of categories and associated synonyms\n",
    "\n",
    "This will enable us to tag the different category of the quote\n",
    "\n",
    "*Note*: This is a restrictive list and some additional content will be added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0ab30d8-b3b6-4bc1-88ec-72f1481ee7a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "matchers = {\"art\": [\"art\", \"paint\", \"draw\", \"museum\"], \\\n",
    "            \"business\": [\"business\", \"finance\", \"economy\", \"commerce\", \"bank\", \"money\", \"trade\"], \\\n",
    "            \"entertainment\":[\"entertainment\"], \n",
    "            \"fashion\":[\"fashion\", \"couture\", \"designer\"], \\\n",
    "            \"medicine\":[\"medicine\", \"health\", \"pharmacy\", \"wellbeing\", \"body\"], \\\n",
    "            \"music\":[\"music\", \"song\", \"album\", \"concert\"], \\\n",
    "            \"politics\":[\"politics\", \"government\"], \\\n",
    "            \"science\":[\"science\", \"research\"], \\\n",
    "            \"sport\": [\"sport\", \"football\", \"athletics\", \"swimming\", \"rugby\", \"tennis\", \"volleyball\", \"ski\"]}\n",
    "\n",
    "# Find general form for categories and words\n",
    "generalizeDictionary(matchers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239b7084-82c3-4c8e-9ff2-dccc4043ce31",
   "metadata": {},
   "source": [
    "### Extract information from URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "492ea7ce-a7d5-4b2c-bbac-89f4cc0214a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length:  285911\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4363b17c2a9445aa852ac88bf3fe10d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_extract = Chunk_url_extract(df_filtered_final, matchers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd98785c-df8c-4868-9e51-8a06cf3da781",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_PATH + 'df_extract.pkl', 'wb') as output:\n",
    "    pickle.dump(df_extract, output)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c2dc0eb-3fb7-4a6c-8ab4-47c8f1b63407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open pickled dataframe\n",
    "with open(DATA_PATH + 'df_extract.pkl', 'rb') as output:\n",
    "    df_extract = pickle.load(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909a6c00-f6a3-41bf-842d-3e886baccc7a",
   "metadata": {},
   "source": [
    "### Formatting wikidata data of interest\n",
    "Using the Wikidata speakers and label description files provided by TA's, we extract data we need for our project.\n",
    "\n",
    "This includes gender, citizenship, data of birth..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd06fead-2097-4a0b-af14-4778713ebd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Wikidata_speakers = pd.read_parquet(DATA_PATH + 'speaker_attributes.parquet')\n",
    "Wikidata_countries = pd.read_csv(DATA_PATH + 'wikidata_labels_descriptions_quotebank.csv.bz2', compression = 'bz2')\n",
    "\n",
    "\n",
    "Wikidata_utils = formating_wikidata(Wikidata_speakers, Wikidata_countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c27b9e1-4468-4ef6-9044-0ede5a448cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save wikidata utils\n",
    "with open(DATA_PATH + 'Wikidata_utils.pkl', 'wb') as output:\n",
    "    pickle.dump(Wikidata_utils, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e4215f-c86f-464b-8e73-5506b4954f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open file \n",
    "\n",
    "with open(DATA_PATH + 'Wikidata_utils.pkl', 'rb') as input_file:\n",
    "    Wikidata_utils = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c520674f-7931-4674-b82b-a230c465809b",
   "metadata": {},
   "source": [
    "### Add information from wikidata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b11451b-9aa1-4bee-b51c-135d50de99dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Merge the info of wikidata from speaker to the initial dataframe\n",
    "df_merged = merge_quotes_wikidata(Wikidata_utils, df_extract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "798252b1-043a-47e8-9383-6664fcae5302",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_PATH + 'df_year_no_media.pkl', 'wb') as output:\n",
    "    pickle.dump(df_merged, output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ada_project)",
   "language": "python",
   "name": "ada_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
